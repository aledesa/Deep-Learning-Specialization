{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ML Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orthogonalization process**: understanding what to tune to achieve which effect.\n",
    "\n",
    "In orthogonalization, you have some controls, but each control does a specific task and does not affect other controls.\n",
    "\n",
    "* Fit training set well on cost function: bigger networ, Adam optimizer, ...\n",
    "* Fit dev set well on cost function: regularization, bigger train set, ...\n",
    "* Fit test set well on cost function: bigger dev set, ...\n",
    "* Perform well in real world: change dev set, change cost function, ....\n",
    "\n",
    "There are thing that cannot be orthogonalize such as early stopping, as it affects both train and test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single number evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better and faster to set a single number evaluation metric for your project before you start it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "|                | Predicted cat | Predicted non-cat |\n",
    "|----------------|---------------|-------------------|\n",
    "| Actual cat     | 3 (True Positive)         | 2 (False Negative)       |\n",
    "| Actual non-cat | 1  (False Positive)           | 4 (True Negative)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**: percentage of true cats in the recognized result: P = 3/(3 + 1)\n",
    "        \n",
    "**recall**: percentage of true recognition cat of the all cat predictions: R = 3/(3 + 2)\n",
    "        \n",
    "**accuracy**: (3+4)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a precision/recall for evaluation is good in a lot of cases, but separately they don't tell you which algothims is better since generally there is a trade-off between them: one model may have better precision but worse recall.\n",
    "\n",
    "A better thing is to combine precision and recall in one single (real) number evaluation metric. There a metric called **$F_1$ score**, which combines them as a harmonic mean:\n",
    "\n",
    "$$F_1 = 2 * \\frac{precision * recall}{precision + recall} = \\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}$$\n",
    "\n",
    "The highest possible value of $F_1$ is $1$, indicating perfect precision and recall, and the lowest possible value is $0$, if either the precision or the recall is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfying and Optimizing metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its hard sometimes to get a single number evaluation metric. One can choose a single optimizing metric and decide that other metrics are satisfying. For example, once the running time is lower than $T$, maximize the $F_1$ score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/dev/test distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dev and test sets have to come from the same distribution.\n",
    "\n",
    "* Choose dev set and test set to reflect data you expect to get in the future and consider important to do well on.\n",
    "\n",
    "* Setting up the dev set, as well as the validation metric is really defining what target you want to aim at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the dev and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An old way of splitting the data was 70% training, 30% test or 60% training, 20% dev, 20% test. The old way was valid for a number of examples <100.000.\n",
    "\n",
    "In the modern deep learning if you have a million or more examples a reasonable split would be 98% training, 1% dev, 1% test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to change dev/test sets and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose your evaluation metric is the simple error:\n",
    "\n",
    "$$\\text{Error} = \\frac{1}{m_{dev}} \\sum_{i=1}^{m_{dev}} \\mathcal{I}\\{y_{\\text{pred}}^{(i)} \\neq y^{(i)}\\}$$\n",
    "\n",
    "If you don't want to misclassify a particular group of input (e.g. porn images) you can increase the weight of the error for that group of observations:\n",
    "\n",
    "$$\\text{Error} = \\frac{1}{\\sum_i w^{(i)}} \\sum_{i=1}^{m_{dev}} w^{(i)} \\mathcal{I}\\{y_{\\text{pred}}^{(i)} \\neq y^{(i)}\\}$$\n",
    "\n",
    "where $w^{(i)} = 1$ is $x^{(i)}$ is non-porn and $10$ if is porn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually an example of an orthogonalization where you should take a machine learning problem and break it into distinct steps:\n",
    "\n",
    "* Figure out how to define a metric that captures what you want to do - place the target.\n",
    "* Worry about how to actually do well on this metric - how to aim/shoot accurately at the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: if doing well on your metric and on the dev/test set doesn't correspond to doing well in your application, change your metric and/or dev/test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare to human-level performance because of two main reasons:\n",
    "* Because of advances in deep learning, machine learning algorithms are suddenly working much better and so it has become much more feasible in a lot of application areas for machine learning algorithms to actually become competitive with human-level performance.\n",
    "* It turns out that the workflow of designing and building a machine learning system is much more efficient when you're trying to do something that humans can also do.\n",
    "\n",
    "After an algorithm reaches the human level performance the progress and accuracy slow down.\n",
    "\n",
    "It is not possible to surpass what is called *Bayes optimal error*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much error range between human-level error and Bayes optimal error.\n",
    "\n",
    "Humans are quite good at a lot of tasks. So as long as Machine learning is worse than humans, you can:\n",
    "* Get labeled data from humans.\n",
    "* Gain insight from manual error analysis: why did a person get it right?\n",
    "* Better analysis of bias/variance: for some problems lik computer vision it is enough to get close to human performance on train error without the need of doing better, while one can focus on reducing the difference between train and dev error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the human-level error can be considered a proxy for the Bayes optimal error, and the difference between the human error and the training error can be considered **avoidable bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving your model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two fundamental asssumptions of supervised learning:\n",
    "* You can fit the training set pretty well. This is roughly saying that you can achieve low avoidable bias.\n",
    "* The training set performance generalizes pretty well to the dev/test set. This is roughly saying that variance is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve your deep learning supervised system follow these guidelines:\n",
    "* Look at the difference between human level error and the training error - avoidable bias.\n",
    "* Look at the difference between the dev/test set and training set error - Variance.\n",
    "\n",
    "If avoidable bias is large you have these options:\n",
    "* Train bigger model.\n",
    "* Train longer/better optimization algorithm (like Momentum, RMSprop, Adam).\n",
    "* Find better NN architecture/hyperparameters search.\n",
    "\n",
    "If variance is large you have these options:\n",
    "* Get more training data.\n",
    "* Regularization (L2, Dropout, data augmentation).\n",
    "* Find better NN architecture/hyperparameters search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error analysis**: process of manually examining mistakes that your algorithm is making. It can give you insights into what to do next. E.g.:\n",
    "\n",
    "In the cat classification example, if you have 10% error on your dev set and you want to decrease the error. You discovered that some of the mislabeled data are dog pictures that look like cats. Should you try to make your cat classifier do better on dogs (this could take some weeks)?\n",
    "\n",
    "Error analysis approach:\n",
    "* Get 100 mislabeled dev set examples at random.\n",
    "* Count up how many are dogs.\n",
    "    * if 5 of 100 are dogs then training your classifier to do better on dogs will decrease your error up to 9.5% (called ceiling), which can be too little.\n",
    "    * if 50 of 100 are dogs then you could decrease your error up to 5%, which is reasonable and you should work on that.\n",
    "\n",
    "Based on the last example, error analysis helps you to analyze the error before taking an action that could take lot of time with no need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you can evaluate multiple error analysis ideas in parallel and choose the best idea: get 100 mislabeled dev set examples, group the different mistakes and count which groups give you the higher rate of mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up incorrectly labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL algorithms are quite robust to random errors in the training set but less robust to systematic errors. But it's OK to go and fix these labels if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatched training and dev/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing on different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some strategies to follow up when training set distribution differs from dev/test sets distribution. For example you have relatively limited amount of data you really care about and you use huge amount of data similar but not equal to have more observations.\n",
    "\n",
    "* Option one (not recommended): shuffle all the data together and extract randomly training and dev/test sets.\n",
    "  * Advantages: all the sets now come from the same distribution.\n",
    "  * Disadvantages: the other (real world) distribution that was in the dev/test sets will occur less in the new dev/test sets and that might be not what you want to achieve\n",
    "      * $\\rightarrow$ the dev set should be made of what you aim at!\n",
    "\n",
    "* Option two: take some of the dev/test set examples and add them to the training set **but keep the dev/test mostly made by the data you reallt care about**.\n",
    "  * Advantages: the distribution you care about is your target now.\n",
    "  * Disadvantage: the distributions in training and dev/test sets are now different. But you will get a better performance over a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance with mismatched data distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have 1% error on train and 10% error on dev set, this is not necessarely a variance problem because maybe the images in the train set were easier to classify while the images on the dev set were more difficult, i.e. they come from different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One check is to take a small sample of the train set and keep it aside, call it train-dev set. Compute the error on this subsample:\n",
    "* If the error in the train-dev set is 9% (similar to the dev/test set) then is a variance problem.\n",
    "* If the error in the train-dev set is 1.5% (similar to the train set) then is a data mismatch problem between train and dev/test set.\n",
    "\n",
    "$$\\text{human-level} \\underbrace{>}_{\\text{avoidable bias}} \\text{training set error}                                        \\underbrace{>}_{\\text{variance}} \\text{train/dev set error}                                            \\underbrace{>}_{\\text{data mismatch}} \\text{dev error}\n",
    "$$\n",
    "\n",
    "To which one can add\n",
    "\n",
    "$$\\text{dev error}  \\underbrace{>}_{\\text{degree of overfit to dev set}} \\text{test error}$$\n",
    "\n",
    "Is the difference is big (positive) then maybe you need to find a bigger dev set (dev set and test set come from the same distribution, so the only way to do much better on the dev set than the test set, is if you somehow managed to overfit the dev set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing data mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't completely systematic solutions to this, but there some things you could try:\n",
    "* Carry out manual error analysis to try to understand the difference between training and dev/test sets.\n",
    "* Make training data more similar, or collect more data similar to dev/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your goal is to make the training data more similar to your dev set one of the techniques you can use **Artificial data synthesis** that can help you make more training data $\\rightarrow$ very useful in speech recognition systems.\n",
    "\n",
    "Combine some of your training data with something that can convert it to the dev/test set distribution.\n",
    "\n",
    "Examples:\n",
    "* Combine normal audio with car noise to get audio with car noise example.\n",
    "* Generate cars using 3D graphics in a car classification example.\n",
    "\n",
    "Be cautious and bear in mind whether or not you might be accidentally simulating data only from a tiny subset of the space of all possible examples because your NN might overfit these generated data (like particular car noise or a particular design of 3D graphics cars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from multiple tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning consists in using the knowledge you took in a task A and apply it in another task B.\n",
    "\n",
    "For example, you have trained a cat classifier with a lot of data, you can use the part of the trained NN to solve x-ray classification problem.\n",
    "\n",
    "To do transfer learning, delete the last layer of NN and it's weights and:\n",
    "* Option 1: if you have a small data set - keep all the other weights as a fixed weights. Add a new last layer(-s) and initialize the new layer weights and feed the new data $(X,y)$ to the NN and learn the new weights $\\rightarrow$ you only optimize the last weights.\n",
    "* Option 2: if you have enough data you can retrain all the weights.\n",
    "\n",
    "Option 1 and 2 are called fine-tuning and training on task A called pretraining.\n",
    "\n",
    "Transfer learning makes sense if task A and B have the same input $X$ (e.g. image, audio) and you have a lot of data for the task A you are transferring from and relatively less data for the task B your transferring to. Low level features from task A could be helpful for learning task B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a model in which the target variable can take various values at once, for example if the image contains a car, a trafic light, a pedestrian altogether.\n",
    "\n",
    "Unlike *softmax* the target can have multiple labels. In multy-task learning each $y^{(i)}$ has dimension $J$, equal to the number of items that can be recognized in every image, for example $y^{(i)} = [0 1 0 0 1 1]$, and the loss function sums over all observations and all items in $y^{(i)}$:\n",
    "\n",
    "$$\\text{Cost} = \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^J L(\\hat{y}^{(i)}_j, y^{(i)}_j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training $J$ neural networks separately if some of the earlier features in neural network can be shared between the different types of objects, then training one neural network to do $J$ things results in better performance than training $J$ completely separate neural networks to do the $J$ tasks separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works if $y$ isn't complete for some labels:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "Y = &[1 ? 1 ...]\\\\\n",
    "    &[0 0 1 ...]\\\\\n",
    "    &[? 1 ? ...]\n",
    "\\end{matrix}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-task learning makes sense if\n",
    "* Training on a set of tasks that could benefit from having shared lower-level features.\n",
    "* Usually, amount of data you have for each task is quite similar.\n",
    "* Can train a big enough network to do well on all the tasks.\n",
    "\n",
    "If you can train a big enough NN, the performance of the multi-task learning compared to splitting the tasks is better.\n",
    "\n",
    "However, today transfer learning is used more often than multi-task learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some systems have multiple stages to implement. An end-to-end deep learning system implements all these stages with a single NN.\n",
    "\n",
    "Example 1:\n",
    "* Speech recognition system:\n",
    "    * Audio ---> Features --> Phonemes --> Words --> Transcript    # non-end-to-end system\n",
    "    * Audio ------------------------------------------------------> Transcript    # end-to-end deep learning system\n",
    "\n",
    "End-to-end deep learning gives data more freedom, it might not use phonemes when training!\n",
    "\n",
    "To build the end-to-end deep learning system that works well, we need a big dataset (more data then in non end-to-end system). If we have a small dataset the ordinary implementation could work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2:\n",
    "\n",
    "* Face recognition system:\n",
    "    * Image ---------------------> Face recognition    # end-to-end deep learning system\n",
    "    * Image --> Face detection --> Face recognition    # deep learning system - best approach for now\n",
    "\n",
    "In practice, the best approach is the second one for now.\n",
    "\n",
    "The second implementation it's a two steps approach where both parts are implemented using deep learning. Its working well because it's harder to get a lot of pictures with people in front of the camera than getting faces of people and compare them.\n",
    "\n",
    "To train the last step of the second implementation the NN takes two faces as an input and outputs if the two faces are the same person or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3:\n",
    "\n",
    "* Machine translation system:\n",
    "    * English --> Text analysis --> ... --> French    # non-end-to-end system\n",
    "    * English ---------------------------------> French    # end-to-end deep learning system - best approach\n",
    "\n",
    "Here end-to-end deep leaning system works better because we have enough data to build it.\n",
    "\n",
    "Example 4:\n",
    "\n",
    "* Estimating child's age from the x-ray picture of a hand:\n",
    "    * Image --> Bones --> Age    # non-end-to-end system - best approach for now\n",
    "    * Image ----------------> Age    # end-to-end system\n",
    "\n",
    "In this example non-end-to-end system works better because we don't have enough data to train end-to-end system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wheter to use end-to-end deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of end-to-end deep learning:\n",
    "* Let the data speak. By having a pure machine learning approach, your NN learning input from X to Y may be more able to capture whatever statistics are in the data, rather than being forced to reflect human preconceptions For examples, in speech analytics the *phonemes* are just a human construction.\n",
    "* Less hand-designing of components needed.\n",
    "\n",
    "Cons of end-to-end deep learning:\n",
    "* May need a large amount of data.\n",
    "* Excludes potentially useful hand-design components (it helps more on the smaller dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying end-to-end deep learning:\n",
    "\n",
    "* Do you have sufficient data to learn a function of the complexity needed to map $X$ to $Y$?\n",
    "* Use ML/DL to learn some individual components.\n",
    "* When applying supervised learning you should carefully choose what types of $X$ to $Y$ mappings you want to learn depending on what task you can get data for $\\rightarrow$ Sometimes it is better  to use DL only to solve part of a bigger problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
