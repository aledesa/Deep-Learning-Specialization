{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "* The learning rate $\\alpha$\n",
    "* The number of iterations of gradient descent\n",
    "* The number of layers\n",
    "* The number of units in each layer\n",
    "* The activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied ML is an iterative process:\n",
    "\n",
    "**IDEA $\\rightarrow$ CODE $\\rightarrow$ EXPERIMENT $\\rightarrow$ IDEA $\\rightarrow$ ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied deep learning is a very iterative process where you just have to go around this cycle many times to hopefully find a good choice of network for your application. So one of the things that determine how quickly you can make progress is how **efficiently** you can go around this cycle. And setting up your data sets well in terms of your **train, development and test sets** can make you much more efficient at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You keep on training algorithms on your training sets.\n",
    "* Use your dev set or your hold-out cross validation set to see which of many different models performs best on your dev set.\n",
    "* And then after having done this long enough, when you have a final model that you want to evaluate, you can take the best model you have found and evaluate it on your test set. In order to get an unbiased estimate of how well your algorithm is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was common practice to take all your data and split it according to maybe a 70/30% in terms of a people often talk about the 70/30 train test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in the modern big data era, where, for example, you might have a million examples in total, then the trend is that your dev and test sets have been becoming a much smaller percentage of the total. Because remember, the goal of the dev set or the development set is that you're going to test different algorithms on it and see which algorithm works better. So the dev set just needs to be big enough for you to evaluate, say, two different algorithm choices or ten different algorithm choices and quickly decide which one is doing better. For example, if you have a million examples, if you need just 10,000 for your dev and 10,000 for your test, your ratio will be 1% of 1 million so you'll have 98% train, 1% dev, 1% test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other trend we're seeing in the era of modern deep learning is that more and more people train on **mismatched train and test distributions**. So maybe your training set has a lot of pictures crawled off the Internet but the dev and test sets are pictures uploaded by users. Turns out a lot of webpages have very high resolution, very professional, very nicely framed pictures of cats. But maybe your users are uploading blurrier, lower res images just taken with a cell phone camera in a more casual condition. And so these two distributions of data may be different. The rule of thumb I'd encourage you to follow in this case is to make sure that the dev and test sets come from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias / Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a model $Y = f(x) + \\epsilon$, the expected MSE can be decomposed into the squared *bias* of $\\hat{f}(x)$, the *variance* of $\\hat{f}(x)$ and the variance of the error term $\\epsilon$:\n",
    "\n",
    "$$E\\big[\\big(Y-\\hat{f}(x)\\big)^2\\big] = \\underbrace{\\big(E\\big[\\hat{f}(x)\\big]-f(x)\\big)^2}_{\\text{bias}^2} + \\underbrace{E\\big[\\big(\\hat{f}(x)-E[\\hat{f}(x)]\\big)^2\\big]}_{\\text{variance}} + \\sigma^2_{\\epsilon}$$\n",
    "\n",
    "* Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between $Y$ and $X1, X2, ..., Xp$. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of $f$. $\\rightarrow$ **underfitting**\n",
    "\n",
    "* The *variance* of $\\hat{f}$ refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\\hat{f}$. But ideally the estimate for $f$ should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$. $\\rightarrow$ **overfitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* High bias?  $\\rightarrow$ Bigger network (more hidden layers, more hidden units), different architectures\n",
    "* High variance?  $\\rightarrow$  More training data, **regularization**\n",
    "\n",
    "Regarding to the **bias-variance tradeoff**, in deep learning generally one solution does not negatively affect the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as **weight decay**, examples of regularization in linear regression are the Lasso or the Ridge regression:\n",
    "\n",
    "$$min_{w,b}J(w,b)=\\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}-y^{(i)})+\\frac{\\lambda}{2m}||w||_2^2$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter and should be chosen using CV or the dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a neural network the cost function becomes\n",
    "\n",
    "$$J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y}^{(i)}-y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L||w^{[l]}||_F^2$$\n",
    "\n",
    "where $||w^{[l]}||_F^2 = \\sum_{i=1}^{n^{[l]}} \\sum_{j=1}^{n^{[lâˆ’1]}}(w_{i,j}^{[l]})^2$ is the *Frobenius norm* of matrix $w^{[l]}$, of dimension $n^{[l]} \\times n^{[l-1]}$.\n",
    "\n",
    "Remember: $\\underbrace{x^{(i)}}_{n^{x} \\times 1} \\rightarrow \\underbrace{\\overbrace{w^{[1]}}^{n^{[1]} \\times n^{[x]}}x^{(i)}+b^{[1]}}_{n^{[1]} \\times 1} \\rightarrow  \\underbrace{\\overbrace{w^{[2]}}^{n^{[2]} \\times n^{[1]}}a^{[1](i)}+b^{[2]}}_{n^{[2]} \\times 1} \\rightarrow ...$\n",
    "\n",
    "Intuition: in this way the update of $w^{[l]}$ during back-propagation is *smaller*. Each hidden unit has a smaller effect: higher $\\lambda \\rightarrow$ smaller $w^{[l]} \\rightarrow$ smaller $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]} \\rightarrow$ the activation function is more linear $\\rightarrow$ closer to a linear regression model.\n",
    "\n",
    "<img src=\"linear_af.PNG\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T09:28:30.920386Z",
     "start_time": "2020-09-12T09:28:30.916383Z"
    }
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout** consists in randomly set hidden units to zero in order to reduce overfitting:\n",
    "\n",
    "<img src=\"dropout.png\" width=\"600px\" />\n",
    "\n",
    "The probability can change for every hidden layer.\n",
    "\n",
    "**Inverted dropout** consists in dividing the remaing hidden units by the probability of being kept, let's say $80\\%$. This is done because in $z^{[l]} = w^{[l]}a^{[l-1]}+b^{[l]}$ the $a^{[l-1]}$ has been reduced by $20\\%$, hence using $a^{[l-1]}/0,8$ makes sure than the expected value of $a^{[l-1]}$ is the same.\n",
    "\n",
    "But notice that in the test time you're not using dropout, you're not flipping coins to decide which hidden units to eliminate. And that's because when you are making predictions at the test time, you don't really want your output to be random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T14:49:50.980799Z",
     "start_time": "2020-09-12T14:49:49.324847Z"
    }
   },
   "source": [
    "If getting additional data is costly, taking random distortions and translations of the image you have could augment your data set and make additional fake training examples.\n",
    "\n",
    "<img src=\"data_aug.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping** consists in stopping the minimization of the cost function (on the train set) before it gets to the minimum.\n",
    "\n",
    "The problem with this method is that we're mixing optimization and reducing overfitting which are two different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the features have very different scale so have the weights $w^{[l]}$. In this case the cost function is \"elongated\" and the gradient descent may need longer steps to finally find the minimum.\n",
    "\n",
    "<img src=\"normalize.PNG\" width=\"600\" />\n",
    "\n",
    "If you scale the train set by subtracting the mean and dividing by the standard deviation then you have to scale the test set with the same values. They have to go by the same transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **deep** networks the gradient may decrease or increase exponentially.\n",
    "\n",
    "A partial solution to this problem is a careful choice of the random initialization. For example you can set the variance equal to $\\frac{1}{n^{[l-1]}}$ in case of tanh activation function (*Xavier initialization*) or $\\frac{2}{n^{[l-1]}}$ in case of ReLu activation function, or $\\frac{2}{n^{[l-1]}+n^{[l]}}$. For example:\n",
    "\n",
    "$$w^{[l]} = np.random.rand(shape)*np.sqrt\\bigg(\\frac{2}{n^{[l-1]}}\\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that instead of computing the gradient on the whole training sample of dimension $(n_x,m)$ one can split the observations in train set in many *batches* of smaller dimension $X^{\\{1\\}}, X^{\\{2\\}}, ...$ and $y^{\\{1\\}}, y^{\\{2\\}}, ...$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epoch**: the number of times when the complete dataset is passed forward and backward by the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each layer $l=1,...L$ the parameters are $W^{[l]}$ of dimension $(n^{[l]} \\times n^{[l-1]})$, where $n^{[0]}=k$.\n",
    "\n",
    "It works as follows:\n",
    "\n",
    "a) First randomly initialize the parameters $W^{[l]}$ for $l=1,...L$.\n",
    "\n",
    "b) Take a sample of length $p\\in[1,m]$ of my training data, denoted by $(X^{(1)},y^{(1)})$ for sample number $1$.\n",
    "\n",
    "c) Compute the cost $J^{(1)}(W)$ with the first initialization of the parameters and the first sample of the train data.\n",
    "\n",
    "d) In back-propagation update the parameters for $l=L,...1$ according to a learning rate $\\alpha$:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } \\frac{\\partial J^{(1)}(W)}{\\partial W^{[l]}}$$\n",
    "\n",
    "This is one step of the gradient descent with one sample of the train data. Now repeat step (c) and (d) with the \"new\" $W^{[l]}$ on a second sample of the train data $(X^{(2)},y^{(2)})$ and so on. It will continue until convergence when every update in the gradient descent is done with different samples of the train data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually 3 main \"kind\" of Gradient Descent:\n",
    "\n",
    "* Batch Gradient Descent: if the training data can fit in memory (RAM / VRAM) the choice is on Batch Gradient Descent. In this case the batch size is equal to the entire dataset. This means that the model is updated only when all the dataset is passed.\n",
    "\n",
    "* Stochastic Gradient Descent: the batch size is equal to 1. This means that the model is updated with only a training instance at time.\n",
    "\n",
    "* Mini-batch Gradient Descent: the batch size is equal to a value $p \\in (1,m)$. This means that the model is updated per batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thumb rule is to use batch gradient descent if you can fit all the dataset in memory. On the contrary, depending on the instance size, the choice will be a mini-batch gradient descent with a fixed size batch that can fit entirely in memory. Usually when you use the mini-batch gradient descent the error convergence will be more noisy compared to batch gradient descent, because of the content variability of the batches.\n",
    "\n",
    "<img src=\"gd.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exponential weighted averages:**\n",
    "\n",
    "Consider a series $\\theta_1, \\theta_1, ...,\\theta_n$ and let $v_t = \\beta v_{t-1} + (1-\\beta) \\theta_t$, with $v_1 = 0$. The value $v_t$ is approximately the average over the last $\\frac{1}{1-\\beta}$ observations of $\\theta_t$, for example when $\\beta=0.9$ $v_t$ i the average of the last 10 $\\theta_t$. The larger is $\\beta$ and the smoother is the series of $v_t$.\n",
    "\n",
    "Since at the beginning $v_t$ is artificially close to zero for a few iterations, one can correct the update using $v^*_t = \\frac{v_t}{1-\\beta^t}$. For example, let $\\beta = 0.98$:\n",
    "\n",
    "$$v_0 = 0$$\n",
    "$$v_1 = 0.02 \\theta_1$$\n",
    "$$v_2 = 0.98 v_1 + 0.002 \\theta_2 = 0.0196 \\theta_1 + 0.02 \\theta_2$$\n",
    "while $(1-\\beta^2) = 0.0396$ so that \n",
    "$$v^*_2 = \\frac{0.0196 \\theta_1 + 0.02 \\theta_2}{0.0396}$$\n",
    "which is a weighted average of the two $\\theta_s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid oscillations in the gradiente descent (blue) or overshooting (purple), the idea is to apply this smoothing mechanism in the update of the weights (red).\n",
    "\n",
    "<img src=\"gd_momentum.png\" width=\"600\" />\n",
    "\n",
    "On iteration $t$:\n",
    " \n",
    "$$v_{dW} = \\beta v_{dW} + (1-\\beta)dW$$\n",
    "$$v_{db} = \\beta v_{db} + (1-\\beta)db$$\n",
    "\n",
    "$$W = W - \\alpha v_{dW}$$\n",
    "$$b = b - \\alpha v_{db}$$\n",
    "\n",
    "This add the hyperparameter $\\beta$, generally set as $\\beta = 0.9$ which works as a friction, so that is as the average of the last 10 iterations. The matrix $v_{dW}$ and $v_{db}$ are initialized as zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T20:51:16.715544Z",
     "start_time": "2020-09-21T20:51:16.712542Z"
    }
   },
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Prop is another algorithm to slow down the update of the parameters in order to make it more stable and reach the minimum faster. Applied to mini-batches at every iteration you compute $dW$ and $db$ and update the parameters in the following way:\n",
    "\n",
    "$$S_{dW} = \\beta S_{dW} + (1-\\beta) dW^2$$\n",
    "$$S_{db} = \\beta S_{db} + (1-\\beta) db^2$$\n",
    "\n",
    "$$W = W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}}$$\n",
    "$$b = b - \\alpha \\frac{db}{\\sqrt{S_{db}}}$$\n",
    "\n",
    "So that the larger the derivative of the gradient (numerator), the larger is the adjustment to slow it down (denominator). One can add $\\epsilon =10^{-8}$ to $S_{dW}$ so that it is different from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam stands for Adaptive moment estimation. It combines momentum and RMSprop on mini-batch gradient descent. On every mini-batch step:\n",
    "\n",
    "* Compute $dW$, $db$ with the current mini-batch\n",
    "* Let $v_{dW} = \\beta_1 v_{dW} + (1-\\beta_1) dW$\n",
    "* Let $S_{dW} = \\beta_2 S_{dW} + (1-\\beta_2) dW^2$\n",
    "* Let $v^{*}_{dW} = \\frac{v_{dW}}{(1-\\beta_1^2)}$ as we correct the bias of the exponentially weighted average\n",
    "* Let $S^{*}_{dW} = \\frac{S_{dW}}{(1-\\beta_2^2)}$ same as above\n",
    "* Let $W = W - \\alpha \\frac{v^{*}_{dW}}{\\sqrt{S^{*}_{dW}}}$\n",
    "\n",
    "and the same for $S_{db}$.\n",
    "\n",
    "There are other parameters: generally the learning rate $\\alpha$ needs to be tunes, $dW$ is $\\beta_1 = 0.9$ and $\\beta_2=0.999$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that might help speed up your learning algorithm, is to slowly reduce your learning rate over time.\n",
    "\n",
    "Maybe a mini-batch has just 64, 128 examples. Then as you iterate, your steps will be a little bit noisy. And it will tend towards the minimum, but it won't exactly converge. But your algorithm might just end up wandering around (blue), and never really converge, because you're using some fixed value for alpha. And there's just some noise in your different mini-batches. Alternatively, if $\\alpha$ gets smaller, your steps you take will be slower and smaller. And so you end up oscillating in a tighter region around this minimum (green), rather than wandering far away.\n",
    "\n",
    "<img src=\"lrd.png\" width=\"500\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\alpha_0 = 0.2$ and set\n",
    "\n",
    "$$\\alpha = \\frac{1}{1+\\text{deacy_rate}*\\text{epoch}} \\alpha_0$$\n",
    "\n",
    "Or other versions of exponential decay:\n",
    "\n",
    "$$\\alpha = 0.95^{\\text{epoch}}*\\alpha_0$$\n",
    "\n",
    "$$\\alpha = \\frac{k}{\\sqrt{\\text{epoch}}}*\\alpha_0$$\n",
    "\n",
    "$$\\alpha = \\frac{k}{\\sqrt{t}}*\\alpha_0$$ where $k$ is a constant and $t$ is the number of the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Optima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high dimensionality is very unlikely to be stuck in local optima. Generally if the derivative is zero it's a saddle point.\n",
    "\n",
    "The problems are **plateaus** which are regions where the gradien is clode to zero in a vast area. This slows down the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoriths like momentum, RMSprop and Adam can really help the learning algorithm\n",
    "\n",
    "<img src=\"gd_performance.gif\" width=\"400\" />  \n",
    " \n",
    "With the case of saddle point, RMSprop(black line) goes straight down, it doesnâ€™t really matter how small the gradients are, RMSprop scales the learning rate so the algorithms goes through saddle point faster than most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\alpha$: learning rate $\\rightarrow$ **most important**\n",
    "* $\\beta$: for momentum $\\rightarrow$ **second importance** ($\\beta=0.9$)\n",
    "* $\\beta_1$, $\\beta_2$, $\\epsilon$: for Adam $\\rightarrow$ **last importance** ($\\beta_1 = 0.9, \\beta_2=0.999, \\epsilon = 10^{-8}$)\n",
    "* \\# of layers $\\rightarrow$ **third importance**\n",
    "* \\# of hidden units $\\rightarrow$ **second importance**\n",
    "* ?learning rate decay $\\rightarrow$ **third importance**\n",
    "* mini-batch size $\\rightarrow$ **second importance**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random search instead of grid search allows you to explore more parameters at the same cost (time). You can run an additional random search in a finer area where \"good\" parameters were found $\\rightarrow$ *coarse to fine search*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to sample using an appropriate scale for the hyperparameters:\n",
    "\n",
    "If $\\alpha \\in [0.0001,1]$ the picking uniformly in this interval means that $90\\%$ we're picking a number between $0.1$ and $1$. It is better to pick uniformly on a logarithmic scale, so that the intervals $[0.0001,0.001], [0.001,0.01], [0.01,0.1], \\text{ and } [0.1,1]$ have the same length:\n",
    "\n",
    "    r = -4 * np.random.rand() # r in [-4,0]\n",
    "    alpha = 10^r # alpha in [0.0001,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T09:44:12.918308Z",
     "start_time": "2020-09-27T09:44:12.913307Z"
    }
   },
   "source": [
    "For exponentually weighted averages $\\beta \\in [0.9,0.999]$, equivalent $1-\\beta \\in [0.001,0.1]$, therefore we set $\\beta = 1-10^{(-r*2-1)}$, with\n",
    "\n",
    "    r = np.random.rand()\n",
    "\n",
    "It is important to avoid sampling from a linear scale because when $\\beta$ is close to 1, the sensitivity of the results you get changes a lot, even with very small changes to beta. that is if $\\beta$ goes from $0.9$ to $0.9005$ (averaging over roughly 10 values), it's no big deal, this is hardly any change in your results. But if $\\beta$ goes from $0.999$ to $0.9995$ (averaging over roughly 1000 or 2000 values), this will have a huge impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization (BN) makes your hyperparameter search problem much easier, makes your neural network much more robust. \n",
    "\n",
    "As normalizing the input features helps learning because it \"regularizes\" the shape of the optimal region, the same can be applied to every hidden layer of the neural network.\n",
    "\n",
    "In order not to have units with zero mean and unit variance we apply  transformation:\n",
    "\n",
    "$$z^{[l](i)} = w^{[l]}a^{[l-1](i)}$$\n",
    "$$z^{[l](i)}_{\\text{norm}} = \\frac{z^{[l](i)}-\\mu}{\\sqrt{\\sigma+\\epsilon}}$$\n",
    "$$\\tilde{z}^{[l](i)} = \\gamma^{[l]} z^{[l](i)}_{\\text{norm}} + \\beta^{[l]}$$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are parameters to be learned that fix the mean and variance of each hidden layer. Notice that the parameter $b^{[l]}$ is not included anymore because when subtracting the mean it would disappear, so it is enough $\\beta^{[l]}$. So in backpropagation you have to use an optimization algorithm to update them:\n",
    "\n",
    "$$\\beta^{[l]} = \\beta^{[l]} - \\alpha d\\beta^{[l]}$$\n",
    "\n",
    "Generally this is done with mini-batches:\n",
    "\n",
    "* for each mini-batch $t=1,...T$\n",
    "    * compute forward prop $X^{\\{t\\}}$\n",
    "        * in each hidden layer use BN to replace $z^{[l]}$ with $\\tilde{z}^{[l]}$\n",
    "    * use back prop to compute $dw^{[l]}, d\\gamma^{[l]}, d\\beta^{[l]}$\n",
    "    * update parameters $w, \\gamma \\text{ and } \\beta$ with GD, RMSprop or Adam with a learning rate $\\alpha$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization helps also to generalize a model in case of a shift in the distributiono of the input features $\\rightarrow$ **covariate shift**\n",
    "\n",
    "Going through a NN, from the perspective of hidden layer $l$, the hidden unit values of the previous layer $a^{[l-1]}$ are changing all the time, and so it's suffering from the problem of covariate shift. So what batch norm does, is it reduces the amount that the distribution of these hidden unit values shifts around.\n",
    "\n",
    "It also contributes to regularization since in a mini-batch GD each mean and variance are computed on the mini-batch so this add some noise within that mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, at training for each mini-batch $i$ of length $m$:\n",
    "* $\\mu = \\frac{1}{m}\\sum_i z^{(i)}$\n",
    "* $\\sigma^2 = \\frac{1}{m}\\sum_i (z^{(i)}-\\mu)^2$\n",
    "* $z^{(i)}_{\\text{norm}} = \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$\n",
    "* $\\tilde{z}^{(i)} = \\gamma z^{(i)}_{\\text{norm}} + \\beta$\n",
    "\n",
    "But at test time you may not have the same mini-batch number, and with a limited number of observation using that mean and variance can be suboptimal.\n",
    "\n",
    "An alternative is to compute $\\mu$ and $\\sigma$ as an exponentially weighted average for each layer across the mini-batches and then use those estimated value in test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multi-class classification problem with $C$ classes the last layer $L$ has $C$ hidden units ($\\hat{y}_i = C \\times 1$) where each unit estimates the probability of being a class: $P(Y=c|X)$.\n",
    "\n",
    "This last layer $L$ is called **softmax layer**. Once you compute $z^{[L]} = w^{[L]} a^{[L-1]}+b^{[L]}$ it uses the activation function $t = e^{(z^{[L]})}$ and tehn computes\n",
    "\n",
    "$$\\hat{y} = a^{[L]} = \\frac{e^{(z^{[L]})}}{\\sum_{j=1}^Ct_j}$$\n",
    "\n",
    "Since the sum of the units of $a^{[L]}$ should be $1$. The difference with the other activation functions is that it takes a vector and returns a vector (instead of $\\mathbb{R} \\rightarrow [0,1]$).\n",
    "\n",
    "Softmax regression is a generalization of logistic regrassion: is $C=2$ it would be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax regression uses the loss function over teh classes:\n",
    "\n",
    "$$L(\\hat{y},y) = -\\sum_{j=1}^C y_j \\log(\\hat{y_j})$$\n",
    "\n",
    "where, for example $y = [0 1 0 0]$ (second class) it becomes $-\\log(\\hat{y}_2)$, so it tries to increase $\\hat{y}_2$, i.e. the probability of $y$ being the second class.\n",
    "\n",
    "The cost function is defined as \n",
    "\n",
    "$$J(w^{[1]},b^{[1]},...) = \\frac{1}{n}\\sum_{i=1}^m L(\\hat{y}^{(i)},y^{(i)})$$\n",
    "\n",
    "Notice that $y$ is a matrix $C \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:38.146351Z",
     "start_time": "2020-10-04T10:11:45.206747Z"
    }
   },
   "outputs": [],
   "source": [
    "# conda create -n myenv37tf python=3.7 tensorflow=1\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a cost function $J(w) = w^2-10w+25$\n",
    "\n",
    "It has minimum at $w^* = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:40.596677Z",
     "start_time": "2020-10-04T10:13:38.155352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alessandro\\anaconda3\\envs\\myenv37tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# let's define the parameter\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "# cost = tf.add(tf.add(w**2, tf.multiply(-10.,w)),25) # old syntax\n",
    "cost = w**2-10*w+25\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:43.665625Z",
     "start_time": "2020-10-04T10:13:40.598680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "session.run(w) # to initialize teh parameters\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:44.552437Z",
     "start_time": "2020-10-04T10:13:43.668628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "# run one step of GD and evaluate w\n",
    "session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:44.755581Z",
     "start_time": "2020-10-04T10:13:44.560451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "# run one thousand step of GD and evaluate w\n",
    "for i in range(1000):\n",
    "    session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only necessary to implement for-prop with a cost function because Tensorflow is able to do the back-prop by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use now some input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:45.091424Z",
     "start_time": "2020-10-04T10:13:44.757581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "coefficient = np.array([[1.],[-10],[25.]])\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3,1]) # A placeholder is an object whose value you can specify only later.\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "session.run(w) # to initialize the parameters\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:45.658551Z",
     "start_time": "2020-10-04T10:13:45.093426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "# run one step of GD and evaluate w\n",
    "session.run(train, feed_dict={x:coefficient}) # To specify values for a placeholder, you can pass in values by using a \"feed dictionary\"\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:13:45.953636Z",
     "start_time": "2020-10-04T10:13:45.663554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "# run one thousand step of GD and evaluate w\n",
    "for i in range(1000):\n",
    "    session.run(train, feed_dict={x:coefficient})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can use this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-04T10:15:18.709919Z",
     "start_time": "2020-10-04T10:15:18.590834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "coefficient = np.array([[1.],[-10],[25.]])\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3,1]) # input data\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:                    # Create a session and print the output\n",
    "    session.run(init)                            # Initializes the variables\n",
    "    session.run(train, feed_dict={x:coefficient})\n",
    "    print(session.run(w))                     # Prints the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
